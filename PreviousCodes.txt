# Create normalizers for L-1 and L-2
normalizer_l1 = Normalizer(norm='l1')
normalizer_l2 = Normalizer(norm='l2')

# Normalize data using L-1 and L-2
normalized_data_l1 = normalizer_l1.fit_transform(X_imputed)
normalized_data_l2 = normalizer_l2.fit_transform(X_imputed)



# Observe the normalized data
print("\n\nOriginal data:\n", X[:5])  # Print the first 5 rows

print("\nL-1 normalized data:\n", normalized_data_l1[:5])
print("\nL-1 norm sum of each column:\n", normalized_data_l1.sum(axis=0))  # Check sum of absolute values

print("\n\nL-2 normalized data:\n", normalized_data_l2[:5])
print("\nL-2 norm sum of squares of each column:\n", (normalized_data_l2**2).sum(axis=0))  # Check sum of squares



#initialize it
scaler = StandardScaler()
#calculate all the necessary data to perform the standarization
scaler.fit(numeric_titanic)
#apply the standarizer to the data
titanic_standardized = pd.DataFrame(scaler.transform(numeric_titanic),columns = numeric_cols)
print(titanic_standardized.std()**2)

print("\n\n\n")
print(titanic_standardized.head())

print("\n\n\n")

#initialize it
scaler = MinMaxScaler()
#calculate all the necessary data to perform the normalization
scaler.fit(X_imputed)
#apply the standarizer to the data
titanic_normalized = pd.DataFrame(scaler.transform(X_imputed),columns = numeric_cols)
print(titanic_normalized.std()**2) # seems better

print("\n\n\n\n")

print(titanic_normalized.corr())



"""
# MinMax Scaling
norm = MinMaxScaler(feature_range=(0,1)).fit(X_imputed)
X_minmax = pd.DataFrame(norm.transform(X_imputed), columns=X.columns)

# Standardization
scale = StandardScaler().fit(X_imputed)
X_scaled = pd.DataFrame(scale.transform(X_imputed), columns=X.columns)

print(X_minmax.describe().round(3))
print("\n\n")
print(X_scaled.describe().round(3))
"""


def binarize_lonliness(row):
    if row['SibSp']+row['Parch']==0:
        return 1 # Alone
    elif row['SibSp']+row['Parch']>0:
        return 0 # Not Alone


titanic['IsAlone'] = titanic.apply(binarize_lonliness, axis=1)


SelectKBest

# Feature selection using chi-squared for classification
skb_object = SelectKBest(f_classif, k=3)
titanic_new = skb_object.fit_transform(titanic[numeric_cols], target)

feature1 = titanic_new[:, 0]
feature2 = titanic_new[:, 1]
feature3 = titanic_new[:, 2]

#here we are drawing each sample with a different color
#the easier it is to separate the classes, the better is the performance of the picked features
# Plot all pairwise combinations of the top 3 features
plt.plot(feature1[target==0], feature2[target==0], 'r.')
plt.plot(feature1[target==1], feature2[target==1], 'g.')
plt.plot(feature1[target==0], feature3[target==0], 'b.')
plt.plot(feature1[target==1], feature3[target==1], 'c.')

plt.xlabel(skb_object.get_feature_names_out()[0])
plt.ylabel(skb_object.get_feature_names_out().all())

plt.xlim([np.min(feature1)-1, np.max(feature1)+1])
plt.ylim([np.min(feature2)-1, np.max(feature2)+1])
plt.show()